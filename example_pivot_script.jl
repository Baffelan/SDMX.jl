# SDMX Pivot Transformation Script
# Generated by SDMX.jl on 2025-01-07T15:35:00
# Handles wide-to-long data pivoting for time series

# Note: This example uses Tidier.jl for data transformation syntax.
# Install with: julia -e "using Pkg; Pkg.add(\"Tidier\")"
using DataFrames, CSV, XLSX, Tidier, Statistics, Dates

# === DATA LOADING ===
println("Loading source data with time columns as headers...")
source_data = CSV.read("wide_format_data.csv", DataFrame)
println("Loaded $(nrow(source_data)) rows and $(ncol(source_data)) columns")

# === PIVOT ANALYSIS ===
# Detected time columns: ["Y2018", "Y2019", "Y2020", "Y2021", "Y2022"]
# Pivot strategy: Convert year columns to long format with TIME_PERIOD and OBS_VALUE

println("Analyzing data structure for pivoting...")
# Identify year columns (columns starting with Y followed by 4 digits)
year_columns = filter(col -> occursin(r"^Y\d{4}$", col), names(source_data))
id_columns = setdiff(names(source_data), year_columns)

println("Year columns identified: $(join(year_columns, ", "))")
println("ID columns: $(join(id_columns, ", "))")

# === PIVOT TRANSFORMATIONS ===
println("\nStarting pivot transformation...")

transformed_data = source_data |>
    # Step 1: Convert wide format to long format
    @pivot_longer(
        cols = all_of(year_columns),
        names_to = "year_col", 
        values_to = "raw_value"
    ) |>
    
    # Step 2: Extract year from column names and create TIME_PERIOD
    @mutate(
        TIME_PERIOD = str_extract(year_col, r"\d{4}"),
        year_col = NULL  # Remove the temporary column
    ) |>
    
    # Step 3: Clean and validate observation values
    @mutate(OBS_VALUE = case_when(
        is.na(raw_value) ~ missing,
        raw_value == "" ~ missing,
        raw_value == "n.a." ~ missing,
        raw_value == "..." ~ missing,
        raw_value == "-" ~ missing,
        str_detect(raw_value, "^\\d+\\.?\\d*$") ~ as.numeric(raw_value),
        .default = missing  # TODO: Review non-numeric values
    )) |>
    @select(-raw_value) |>  # Remove temporary raw_value column
    
    # Step 4: Map source columns to SDMX dimensions
    @mutate(
        # Map country names to GEO_PICT codes
        GEO_PICT = case_when(
            country_name == "Fiji" ~ "FJ",
            country_name == "Vanuatu" ~ "VU", 
            country_name == "Samoa" ~ "WS",
            country_name == "Tonga" ~ "TO",
            country_name == "Cook Islands" ~ "CK",
            country_name == "Palau" ~ "PW",
            str_detect(country_name, "Papua") ~ "PG",
            .default = toupper(substr(country_name, 1, 2))  # TODO: Verify country mappings
        ),
        
        # Map indicator names to SDMX codes
        INDICATOR = case_when(
            str_detect(indicator, "GDP") ~ "GDP_PC",
            str_detect(indicator, "Population") ~ "POP_TOT", 
            str_detect(indicator, "Tourism") ~ "TOUR_ARR",
            str_detect(indicator, "Export") ~ "EXP_GOODS",
            str_detect(indicator, "Import") ~ "IMP_GOODS",
            .default = "OTHER"  # TODO: Map remaining indicators
        ),
        
        # Set frequency and other required fields
        FREQ = "A",  # Annual data
        
        # Determine unit of measure based on indicator
        UNIT_MEASURE = case_when(
            str_detect(indicator, "GDP|Export|Import") ~ "USD",
            str_detect(indicator, "Population") ~ "NUMBER",
            str_detect(indicator, "Tourism") ~ "NUMBER", 
            str_detect(indicator, "%|percent|rate") ~ "PERCENT",
            .default = "PURE_NUMB"
        ),
        
        # Set unit multiplier 
        UNIT_MULT = case_when(
            str_detect(indicator, "GDP") ~ "6",  # Millions
            str_detect(indicator, "Population") ~ "3",  # Thousands
            str_detect(indicator, "Export|Import") ~ "3",  # Thousands
            .default = "0"  # Units
        )
    ) |>
    
    # Step 5: Add data quality flags
    @mutate(
        OBS_STATUS = case_when(
            is.na(OBS_VALUE) ~ "M",  # Missing
            data_source == "estimated" ~ "E",  # Estimated
            data_source == "provisional" ~ "P",  # Provisional
            confidence_level == "low" ~ "L",  # Low confidence
            .default = ""  # Normal value
        ),
        
        # Add observation comments for unusual values
        OBS_COMMENT = case_when(
            !is.na(footnote) ~ footnote,
            OBS_STATUS == "E" ~ "Estimated value",
            OBS_STATUS == "P" ~ "Provisional data",
            .default = ""
        )
    ) |>
    
    # Step 6: Filter out invalid records
    @filter(
        !is.na(GEO_PICT),
        !is.na(INDICATOR), 
        !is.na(TIME_PERIOD),
        TIME_PERIOD >= "2015",  # Only recent data
        TIME_PERIOD <= "2023"   # Valid time range
    ) |>
    
    # Step 7: Final column selection and ordering
    @select(
        FREQ, GEO_PICT, INDICATOR, TIME_PERIOD,
        OBS_VALUE, UNIT_MEASURE, UNIT_MULT, 
        OBS_STATUS, OBS_COMMENT
    ) |>
    
    # Step 8: Remove duplicates and sort
    @distinct(FREQ, GEO_PICT, INDICATOR, TIME_PERIOD, .keep_all = true) |>
    @arrange(GEO_PICT, INDICATOR, TIME_PERIOD)

println("Pivot transformation completed!")
println("Transformed from $(length(year_columns)) year columns to long format")
println("Result: $(nrow(transformed_data)) observations")

# === POST-PIVOT VALIDATION ===
println("\n=== PIVOT VALIDATION ===")

# Check if pivot worked correctly
original_obs_count = nrow(source_data) * length(year_columns)
actual_obs_count = nrow(transformed_data)
expected_valid_obs = sum(!ismissing.(transformed_data.OBS_VALUE))

println("Original potential observations: $original_obs_count")
println("Actual observations after pivot: $actual_obs_count")  
println("Valid (non-missing) observations: $expected_valid_obs")
println("Data completeness: $(round(expected_valid_obs/actual_obs_count*100, digits=1))%")

# Validate time period range
time_range = sort(unique(skipmissing(transformed_data.TIME_PERIOD)))
println("Time period range: $(first(time_range)) to $(last(time_range))")

# Check geographic coverage
countries = sort(unique(skipmissing(transformed_data.GEO_PICT)))
println("Geographic coverage: $(length(countries)) countries/territories")
println("Countries: $(join(countries, ", "))")

# Check indicator coverage  
indicators = sort(unique(skipmissing(transformed_data.INDICATOR)))
println("Indicators: $(length(indicators)) unique indicators")
println("Indicator codes: $(join(indicators, ", "))")

# === VALIDATION CHECKS ===
println("\n=== DATA VALIDATION ===")

# Check for required SDMX columns
required_cols = ["FREQ", "GEO_PICT", "INDICATOR", "TIME_PERIOD", "OBS_VALUE"]
missing_cols = setdiff(required_cols, names(transformed_data))
if isempty(missing_cols)
    println("‚úì All required SDMX columns present")
else
    println("‚ùå Missing required columns: $(join(missing_cols, ", "))")
end

# Validate observation values
valid_obs = filter(!ismissing, transformed_data.OBS_VALUE)
if !isempty(valid_obs)
    println("‚úì Observation values: $(length(valid_obs)) valid")
    println("  Range: $(round(minimum(valid_obs), digits=2)) to $(round(maximum(valid_obs), digits=2))")
    
    # Check for negative values that might be invalid
    negative_vals = filter(x -> x < 0, valid_obs)
    if !isempty(negative_vals)
        println("‚ö† Found $(length(negative_vals)) negative values - verify if appropriate")
    end
    
    # Check for extremely large values
    large_vals = filter(x -> x > 1e9, valid_obs)
    if !isempty(large_vals)
        println("‚ö† Found $(length(large_vals)) very large values - verify units")
    end
else
    println("‚ùå No valid observation values found!")
end

# Check TIME_PERIOD format consistency
invalid_time = filter(tp -> !ismissing(tp) && !occursin(r"^\d{4}$", tp), transformed_data.TIME_PERIOD)
if isempty(invalid_time)
    println("‚úì All TIME_PERIOD values follow YYYY format")
else
    println("‚ö† Invalid TIME_PERIOD formats: $(unique(invalid_time))")
end

# === DATA OUTPUT ===
println("\n=== WRITING OUTPUT ===")

# Generate output filename with metadata
output_filename = "sdmx_pivoted_$(Dates.format(now(), "yyyymmdd_HHMMSS")).csv"

# Write SDMX-CSV with proper formatting
CSV.write(output_filename, transformed_data)

println("‚úÖ SDMX-CSV output written to: $output_filename")

# Create a summary report
summary_filename = replace(output_filename, ".csv" => "_summary.txt")
open(summary_filename, "w") do f
    write(f, """SDMX Pivot Transformation Summary
    
Generated: $(now())
Source: Wide format data with year columns
Output: $output_filename

Transformation Details:
- Original format: $(nrow(source_data)) rows √ó $(ncol(source_data)) columns
- Year columns pivoted: $(join(year_columns, ", "))  
- Final format: $(nrow(transformed_data)) observations
- Time range: $(first(time_range)) - $(last(time_range))
- Countries: $(length(countries))
- Indicators: $(length(indicators))
- Data completeness: $(round(expected_valid_obs/actual_obs_count*100, digits=1))%

Quality Flags Used:
- M: Missing data
- E: Estimated values  
- P: Provisional data
- L: Low confidence

Review Required:
- Verify country code mappings
- Confirm indicator code assignments
- Check unit of measure appropriateness
- Validate any negative or extreme values
""")
end

println("‚úÖ Summary report written to: $summary_filename")

# Display sample of transformed data
println("\n=== SAMPLE OUTPUT ===")
println("First 10 observations:")
println(first(transformed_data, 10))

println("\n=== PIVOT TRANSFORMATION COMPLETED ===")
println("üìä Successfully converted wide format to SDMX long format")
println("üîç Review the summary report for data quality insights") 
println("‚úèÔ∏è  Address TODO items before final submission")
println("üì§ Output ready for SDMX validation and submission")