# SDMX Data Transformation Script
# Generated by SDMX.jl on 2025-01-07T15:30:00
# Source: pacific_tourism_data.csv
# Target: SPC:DF_BP50

# Note: This example uses Tidier.jl for data transformation syntax.
# Install with: julia -e "using Pkg; Pkg.add(\"Tidier\")"
using DataFrames, CSV, XLSX, Tidier, Statistics, Dates

# === DATA LOADING ===
println("Loading source data...")
source_data = CSV.read("pacific_tourism_data.csv", DataFrame)
println("Loaded $(nrow(source_data)) rows and $(ncol(source_data)) columns")

# === DATA EXPLORATION ===
println("Source data structure:")
println(describe(source_data))
println("\nFirst few rows:")
println(first(source_data, 3))

println("\nColumn analysis:")
for col in names(source_data)
    missing_pct = round(sum(ismissing.(source_data[!, col])) / nrow(source_data) * 100, digits=1)
    println("  $col: $(eltype(source_data[!, col])), $missing_pct% missing")
end

# === DATA TRANSFORMATIONS ===
println("\nStarting data transformations...")

transformed_data = source_data |>
    # Step 1: Standardize country codes to SDMX GEO_PICT format
    @mutate(GEO_PICT = case_when(
        country == "Fiji" ~ "FJ",
        country == "Tuvalu" ~ "TV", 
        country == "Vanuatu" ~ "VU",
        country == "Samoa" ~ "WS",
        country == "Tonga" ~ "TO",
        country == "Solomon Islands" ~ "SB",
        country == "Palau" ~ "PW",
        country == "Papua New Guinea" ~ "PG",
        country == "Cook Islands" ~ "CK",
        country == "Niue" ~ "NU",
        country == "French Polynesia" ~ "PF",
        country == "New Caledonia" ~ "NC",
        .default = country  # TODO: Review unmapped countries
    )) |>
    
    # Step 2: Convert year to TIME_PERIOD format
    @mutate(TIME_PERIOD = case_when(
        !is.na(year) ~ as.character(year),
        !is.na(reporting_period) ~ as.character(reporting_period),
        .default = "9999"  # TODO: Handle missing time periods
    )) |>
    
    # Step 3: Map tourism indicators to SDMX codes
    @mutate(INDICATOR = case_when(
        str_detect(indicator_name, "Arrivals") ~ "BP50_01",
        str_detect(indicator_name, "Tourism receipts") ~ "BP50_02", 
        str_detect(indicator_name, "Average stay") ~ "BP50_03",
        str_detect(indicator_name, "Expenditure") ~ "BP50_04",
        .default = "BP50_99"  # TODO: Review unmapped indicators
    )) |>
    
    # Step 4: Clean and validate observation values
    @mutate(OBS_VALUE = case_when(
        is.na(value) ~ missing,
        value < 0 ~ missing,  # Remove negative values
        value > 1e12 ~ missing,  # Remove unrealistic large values
        .default = as.numeric(value)
    )) |>
    
    # Step 5: Add required SDMX dimensions with default values
    @mutate(
        FREQ = "A",  # Annual frequency
        UNIT_MEASURE = case_when(
            str_detect(indicator_name, "Arrivals") ~ "NUMBER",
            str_detect(indicator_name, "receipts|Expenditure") ~ "USD", 
            str_detect(indicator_name, "stay") ~ "DAYS",
            .default = "PURE_NUMB"
        ),
        UNIT_MULT = case_when(
            str_detect(indicator_name, "receipts|Expenditure") ~ "3",  # Thousands
            .default = "0"  # Units
        )
    ) |>
    
    # Step 6: Add observation status flags
    @mutate(OBS_STATUS = case_when(
        is.na(OBS_VALUE) ~ "M",  # Missing
        data_quality == "provisional" ~ "P",  # Provisional
        data_quality == "estimated" ~ "E",  # Estimated
        .default = ""  # Normal value
    )) |>
    
    # Step 7: Filter and select final SDMX columns
    @filter(!is.na(GEO_PICT), !is.na(TIME_PERIOD), !is.na(INDICATOR)) |>
    @select(
        FREQ, GEO_PICT, INDICATOR, TIME_PERIOD,
        OBS_VALUE, UNIT_MEASURE, UNIT_MULT, OBS_STATUS
    ) |>
    
    # Step 8: Remove any duplicate observations
    @distinct(FREQ, GEO_PICT, INDICATOR, TIME_PERIOD, .keep_all = true)

println("Transformation completed. Result: $(nrow(transformed_data)) rows")

# === DATA VALIDATION ===
println("\nValidation checks:")

# Check required columns are present
required_cols = ["FREQ", "GEO_PICT", "INDICATOR", "TIME_PERIOD", "OBS_VALUE"]
missing_cols = setdiff(required_cols, names(transformed_data))
if !isempty(missing_cols)
    @warn "Missing required columns: $missing_cols"
else
    println("✓ All required columns present")
end

# Check for missing values in key columns
for col in required_cols
    if col in names(transformed_data)
        missing_count = sum(ismissing.(transformed_data[!, col]))
        if missing_count > 0
            println("⚠ Column '$col' has $missing_count missing values")
        else
            println("✓ Column '$col' has no missing values")
        end
    end
end

# Validate GEO_PICT codes (should be ISO 2-letter codes)
invalid_geo = filter(x -> !ismissing(x) && length(x) != 2, transformed_data.GEO_PICT)
if !isempty(invalid_geo)
    println("⚠ Invalid GEO_PICT codes found: $(unique(invalid_geo))")
else
    println("✓ All GEO_PICT codes are valid")
end

# Validate TIME_PERIOD format (should be 4-digit years)
invalid_time = filter(x -> !ismissing(x) && !occursin(r"^\d{4}$", string(x)), transformed_data.TIME_PERIOD)
if !isempty(invalid_time)
    println("⚠ Invalid TIME_PERIOD values found: $(unique(invalid_time))")
else
    println("✓ All TIME_PERIOD values are valid")
end

# Check observation value distribution
valid_obs = filter(!ismissing, transformed_data.OBS_VALUE)
if !isempty(valid_obs)
    println("✓ Observation values: $(length(valid_obs)) valid, range $(minimum(valid_obs)) to $(maximum(valid_obs))")
    
    # Flag potential outliers (values > 3 standard deviations from mean)
    if length(valid_obs) > 10
        mean_val = mean(valid_obs)
        std_val = std(valid_obs)
        outliers = filter(x -> abs(x - mean_val) > 3 * std_val, valid_obs)
        if !isempty(outliers)
            println("⚠ Potential outliers detected: $(length(outliers)) values")
        else
            println("✓ No statistical outliers detected")
        end
    end
else
    @warn "No valid observation values found!"
end

# Summary statistics
println("\n=== TRANSFORMATION SUMMARY ===")
println("Original data: $(nrow(source_data)) rows")
println("Transformed data: $(nrow(transformed_data)) rows") 
println("Data retention: $(round(nrow(transformed_data)/nrow(source_data)*100, digits=1))%")
println("Countries: $(length(unique(skipmissing(transformed_data.GEO_PICT))))")
println("Indicators: $(length(unique(skipmissing(transformed_data.INDICATOR))))")
println("Time periods: $(length(unique(skipmissing(transformed_data.TIME_PERIOD))))")

# === DATA OUTPUT ===
println("\nWriting SDMX-CSV output...")

# Create output filename with timestamp
output_filename = "sdmx_bp50_$(Dates.format(now(), "yyyymmdd_HHMMSS")).csv"

# Write the SDMX-compliant CSV
CSV.write(output_filename, transformed_data)

println("✅ SDMX-CSV output written to: $output_filename")
println("✅ Transformation completed successfully!")

# === POST-PROCESSING RECOMMENDATIONS ===
println("\n=== MANUAL REVIEW REQUIRED ===")
println("TODO items that require your attention:")
println("1. Review country code mappings for any unmapped values")
println("2. Verify indicator code assignments match your data dictionary") 
println("3. Check TIME_PERIOD format consistency")
println("4. Validate UNIT_MEASURE assignments for each indicator")
println("5. Review any flagged outliers or data quality issues")
println("6. Test the output file with your SDMX validation tools")
println("\n=== NEXT STEPS ===")
println("1. Load and inspect the generated CSV file")
println("2. Run additional validation against your SDMX schema")
println("3. Adjust transformations based on domain knowledge") 
println("4. Document any manual changes for reproducibility")

# Optional: Display sample of final data
println("\n=== SAMPLE OUTPUT ===")
println(first(transformed_data, 5))